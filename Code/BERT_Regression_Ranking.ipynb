{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23610,
     "status": "ok",
     "timestamp": 1689075148249,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "dCADxe0IfFXv",
    "outputId": "abfaa914-5363-42ee-8b32-1e1c7f296963"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install rsmtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdGzPXV8W2AE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from rsmtool.utils.metrics import quadratic_weighted_kappa, difference_of_standardized_means, standardized_mean_difference\n",
    "from rsmtool.fairness_utils import get_fairness_analyses\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N18bh-0rXLea"
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CustomLoss, self).__init__()\n",
    "\n",
    "  def forward(self, predictions, targets, total_epchos, current_epcho):\n",
    "    loss_regression = torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "    preds_smax = F.softmax(predictions, dim=1)\n",
    "    true_smax = F.softmax(targets, dim=1)\n",
    "    preds_smax = preds_smax + 1e-10\n",
    "    preds_log = torch.log(preds_smax)\n",
    "    loss_ListNet = torch.mean(-torch.sum(true_smax * preds_log, dim=1))\n",
    "\n",
    "    gamma = 0.99999\n",
    "    tau = torch.tensor(1/(1+math.exp(gamma*(total_epchos/2-current_epcho))))\n",
    "    loss = tau * loss_regression + (1 - tau) * loss_ListNet\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MruX5jGedf1X"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  prompt_1 = pd.read_csv(path+'Prompt_1.csv')\n",
    "  prompt_2 = pd.read_csv(path+'Prompt_2.csv')\n",
    "  prompt_3 = pd.read_csv(path+'Prompt_3.csv')\n",
    "  prompt_4 = pd.read_csv(path+'Prompt_4.csv')\n",
    "  prompt_5 = pd.read_csv(path+'Prompt_5.csv')\n",
    "  prompt_6 = pd.read_csv(path+'Prompt_6.csv')\n",
    "  prompt_7 = pd.read_csv(path+'Prompt_7.csv')\n",
    "  prompt_8 = pd.read_csv(path+'Prompt_8.csv')\n",
    "  prompt_9 = pd.read_csv(path+'Prompt_9.csv')\n",
    "  prompt_10 = pd.read_csv(path+'Prompt_10.csv')\n",
    "  prompt_11 = pd.read_csv(path+'Prompt_11.csv')\n",
    "  prompt_12 = pd.read_csv(path+'Prompt_12.csv')\n",
    "  return [prompt_1, prompt_2, prompt_3, prompt_4, prompt_5, prompt_6, prompt_7, prompt_8, prompt_9, prompt_10, prompt_11, prompt_12]\n",
    "\n",
    "def split_data(data, fold):\n",
    "    kfold = KFold(n_splits=fold, shuffle=False)\n",
    "    results = []\n",
    "    for train_index, test_index in kfold.split(data):\n",
    "        results.append((train_index, test_index))\n",
    "    return results\n",
    "\n",
    "def get_features(texts, labels):\n",
    "  range_min = 1\n",
    "  range_max = 6\n",
    "  labels = np.asarray((labels-range_min)/(range_max-range_min))\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  for text in texts:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "               text,\n",
    "               max_length = 512,\n",
    "               add_special_tokens = True,\n",
    "               padding = 'max_length',\n",
    "               return_attention_mask = True,\n",
    "               truncation = True,\n",
    "               return_tensors = 'pt',\n",
    "               )\n",
    "    input_ids.append(encoding['input_ids'].squeeze())\n",
    "    attention_masks.append(encoding['attention_mask'].squeeze())\n",
    "  return  input_ids, attention_masks, labels\n",
    "\n",
    "def get_data_loader(input_ids, attention_masks, labels, batch_size):\n",
    "  labels = torch.tensor(labels.reshape(-1))\n",
    "  input_ids = torch.stack(input_ids)\n",
    "  attention_masks = torch.stack(attention_masks)\n",
    "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "  dataloader = DataLoader(\n",
    "               dataset,\n",
    "               sampler = RandomSampler(dataset),\n",
    "               batch_size = batch_size\n",
    "               )\n",
    "  return dataloader\n",
    "\n",
    "def accuracy_evaluation(y_pred, y_test):\n",
    "    qwk = quadratic_weighted_kappa(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    pearson_score = pearsonr(y_test, y_pred).statistic\n",
    "    return qwk, mae, pearson_score\n",
    "\n",
    "def fairness_evaluation(y_pred, y_test, demo_attribute):\n",
    "    df = pd.DataFrame({\"True_Score\":y_test, \"Prediction_Score\":y_pred, \"Demo\":demo_attribute})\n",
    "    results = get_fairness_analyses(df, group=\"Demo\", system_score_column=\"Prediction_Score\", human_score_column=\"True_Score\")[1].values()[3]\n",
    "    population_y_true_observed_sd = np.std(y_test)\n",
    "    population_y_true_observed_mn = np.mean(y_test)\n",
    "    population_y_pred_sd = np.std(y_pred)\n",
    "    population_y_pred_mn = np.mean(y_pred)\n",
    "    y_test_demo_0 = y_test[np.where(demo_attribute==0)]\n",
    "    y_test_demo_1 = y_test[np.where(demo_attribute==1)]\n",
    "    y_pred_demo_0 = y_pred[np.where(demo_attribute==0)]\n",
    "    y_pred_demo_1 = y_pred[np.where(demo_attribute==1)]\n",
    "    SMD_0 = difference_of_standardized_means(y_test_demo_0, y_pred_demo_0, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    SMD_1 = difference_of_standardized_means(y_test_demo_1, y_pred_demo_1, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    diff_mae = mean_absolute_error(y_test_demo_1, y_pred_demo_1) - mean_absolute_error(y_test_demo_0, y_pred_demo_0)\n",
    "    scores = pd.DataFrame({\"SMD_0\":[SMD_0], \"SMD_1\":[SMD_1], \"diff_mae\":[diff_mae]})\n",
    "    return results, scores\n",
    "\n",
    "def covert_label(y_pred):\n",
    "  range_min = 1\n",
    "  range_max = 6\n",
    "  y_orig = [(score*(range_max-range_min)+range_min) for score in y_pred]\n",
    "  return y_orig\n",
    "\n",
    "def get_data_loader_for_test(input_ids, attention_masks, batch_size):\n",
    "  input_ids = torch.stack(input_ids)\n",
    "  attention_masks = torch.stack(attention_masks)\n",
    "  dataset = TensorDataset(input_ids, attention_masks)\n",
    "  dataloader = DataLoader(\n",
    "               dataset,\n",
    "               sampler = SequentialSampler(dataset),\n",
    "               batch_size = batch_size\n",
    "               )\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoUdTsJZOwEx"
   },
   "outputs": [],
   "source": [
    "class BertRegression(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "    self.linear = nn.Linear(768, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = outputs['pooler_output']\n",
    "    regression_output = self.linear(pooled_output)\n",
    "    final_output = self.sigmoid(regression_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiiZtypjR2Uu"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_dataloader, dev_dataloader, epochs, gpu):\n",
    "  best_qwk = float('-inf')\n",
    "  for i in range(0, epochs):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    print(\"Epoch \" + str(i+1))\n",
    "    for it, (input_ids, attention_masks, labels) in tqdm(enumerate(train_dataloader)):\n",
    "      model.zero_grad()\n",
    "      cuda_input_ids = input_ids.cuda(gpu)\n",
    "      cuda_attention_masks = attention_masks.cuda(gpu)\n",
    "      cuda_labels = labels.cuda(gpu)\n",
    "\n",
    "      outputs = model(cuda_input_ids, cuda_attention_masks)\n",
    "\n",
    "      loss = criterion(outputs, cuda_labels.unsqueeze(1), epochs, i+1)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      total_loss += loss.item()\n",
    "      count += 1\n",
    "\n",
    "    qwk = evaluate(model, criterion, dev_dataloader, gpu)\n",
    "    if best_qwk < qwk:\n",
    "        best_qwk = qwk\n",
    "        torch.save(model.state_dict(), '')\n",
    "    print(\"Epoch {} complete, train loss: {}, dev qwk: {}\".format(i+1, total_loss/count, qwk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcHKZu05WcPe"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dev_dataloader, gpu):\n",
    "  model.eval()\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "  with torch.no_grad():\n",
    "    for input_ids, attention_masks, labels in dev_dataloader:\n",
    "      cuda_input_ids = input_ids.cuda(gpu)\n",
    "      cuda_attention_masks = attention_masks.cuda(gpu)\n",
    "      cuda_labels = labels.cuda(gpu)\n",
    "\n",
    "      outputs = model(cuda_input_ids, cuda_attention_masks)\n",
    "\n",
    "      results = outputs.squeeze()\n",
    "      results = results.detach().cpu().numpy()\n",
    "      for result in results:\n",
    "        y_pred.append(result)\n",
    "\n",
    "      labels = cuda_labels.squeeze().detach().cpu().numpy()\n",
    "      for label in labels:\n",
    "        y_true.append(label)\n",
    "  qwk = quadratic_weighted_kappa(y_true, y_pred)\n",
    "  return qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF6D1SdNXiz-"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, gpu):\n",
    "  model.eval()\n",
    "  res = []\n",
    "  with torch.no_grad():\n",
    "    for input_ids, attention_masks in dataloader:\n",
    "      cuda_input_ids = input_ids.cuda(gpu)\n",
    "      cuda_attention_masks = attention_masks.cuda(gpu)\n",
    "\n",
    "      outputs = model(cuda_input_ids, cuda_attention_masks)\n",
    "      results = outputs.squeeze()\n",
    "      results = results.detach().cpu().numpy()\n",
    "      for result in results:\n",
    "        res.append(result)\n",
    "  return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT_DsvNB9fnL"
   },
   "outputs": [],
   "source": [
    "def run_experiment(seed):\n",
    "  df = pd.DataFrame(columns=[\"prompt\", \"fold\", \"quadratic_weighted_kappa\", \"mean_absolute_error\", \"pearson_correlation_coefficient\",\n",
    "                            \"OSA_gender\", \"OSA_gender_p_value\", \"OSD_gender\", \"OSD_gender_p_value\", \"CSD_gender\", \"CSD_gender_p_value\", \"SMD_1_gender\", \"SMD_0_gender\", \"MAED_gender\",\n",
    "                            \"OSA_Economically_disadvantaged\", \"OSA_Economically_disadvantaged_p_value\", \"OSD_Economically_disadvantaged\", \"OSD_Economically_disadvantaged_p_value\", \"CSD_Economically_disadvantaged\", \"CSD_Economically_disadvantaged_p_value\", \"SMD_1_Economically_disadvantaged\", \"SMD_0_Economically_disadvantaged\", \"MAED_Economically_disadvantaged\",\n",
    "                            \"OSA_Disability\", \"OSA_Disability_p_value\", \"OSD_Disability\", \"OSD_Disability_p_value\", \"CSD_Disability\", \"CSD_Disability_p_value\", \"SMD_1_Disability\", \"SMD_0_Disability\", \"MAED_Disability\",\n",
    "                            \"OSA_English_Language_Learner\", \"OSA_English_Language_Learner_p_value\", \"OSD_English_Language_Learner\", \"OSD_English_Language_Learner_p_value\", \"CSD_English_Language_Learner\", \"CSD_English_Language_Learner_p_value\", \"SMD_1_English_Language_Learner\", \"SMD_0_English_Language_Learner\", \"MAED_English_Language_Learner\",\n",
    "                            \"OSA_Race\", \"OSA_Race_p_value\", \"OSD_Race\", \"OSD_Race_p_value\", \"CSD_Race\", \"CSD_Race_p_value\", \"SMD_1_Race\", \"SMD_0_Race\", \"MAED_Race\"])\n",
    "  gpu = 0\n",
    "  criterion = CustomLoss()\n",
    "  epochs = 30\n",
    "  prompts = load_data(\"\")\n",
    "  i = 0\n",
    "  for prompt in prompts:\n",
    "    print(\"Prompt\"+str(i+1))\n",
    "    kfolds = split_data(prompt, 5)\n",
    "    k = 0\n",
    "    for kfold in kfolds:\n",
    "      X_train_all = prompt.iloc[kfold[0]]['Text'].to_numpy()\n",
    "      y_train_all = prompt.iloc[kfold[0]]['Overall'].to_numpy()\n",
    "\n",
    "      X_test = prompt.iloc[kfold[1]]['Text'].to_numpy()\n",
    "      y_test = prompt.iloc[kfold[1]]['Overall'].to_numpy()\n",
    "      test_info = prompt.iloc[kfold[1]]\n",
    "\n",
    "      X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.25, random_state=seed)\n",
    "\n",
    "      input_ids_train, attention_masks_train, labels_train = get_features(X_train, y_train)\n",
    "      input_ids_val, attention_masks_val, labels_val = get_features(X_val, y_val)\n",
    "      input_ids_test, attention_masks_test, labels_test = get_features(X_test, y_test)\n",
    "\n",
    "      train_dataloader = get_data_loader(input_ids_train, attention_masks_train, labels_train, 16)\n",
    "      dev_dataloader = get_data_loader(input_ids_val, attention_masks_val, labels_val, 16)\n",
    "      test_dataloader = get_data_loader_for_test(input_ids_test, attention_masks_test, 16)\n",
    "\n",
    "      model = BertRegression()\n",
    "      model.cuda(gpu)\n",
    "      optimizer = optim.Adam(model.parameters(), lr = 4e-5)\n",
    "\n",
    "      train(model, optimizer, criterion, train_dataloader, dev_dataloader, epochs, gpu)\n",
    "\n",
    "      best_model = BertRegression()\n",
    "      best_model.cuda(gpu)\n",
    "      best_model.load_state_dict(torch.load(''))\n",
    "\n",
    "      y_pred = np.array(covert_label(predict(best_model, test_dataloader, gpu)))\n",
    "      qwk, mae, pearson_score = accuracy_evaluation(y_pred, y_test)\n",
    "      print(str(qwk), str(mae), str(pearson_score))\n",
    "      fairness_part1_Gender, fairness_part2_Gender = fairness_evaluation(y_pred, y_test, test_info['Gender'].to_numpy())\n",
    "      fairness_part1_Economically_disadvantaged, fairness_part2_Economically_disadvantaged = fairness_evaluation(y_pred, y_test, test_info['Economically_disadvantaged'].to_numpy())\n",
    "      fairness_part1_Disability, fairness_part2_Disability = fairness_evaluation(y_pred, y_test, test_info['Disability'].to_numpy())\n",
    "      fairness_part1_English_Language_Learner, fairness_part2_English_Language_Learner = fairness_evaluation(y_pred, y_test, test_info['English_Language_Learner'].to_numpy())\n",
    "      fairness_part1_Race, fairness_part2_Race = fairness_evaluation(y_pred, y_test, test_info['Race_Binary'].to_numpy())\n",
    "      new_row = {\"prompt\" : i+1, \"fold\": k+1, \"quadratic_weighted_kappa\": qwk, \"mean_absolute_error\": mae, \"pearson_correlation_coefficient\": pearson_score,\n",
    "                      \"OSA_gender\": fairness_part1_Gender['Overall score accuracy']['R2'],\n",
    "                      \"OSA_gender_p_value\": fairness_part1_Gender['Overall score accuracy']['sig'],\n",
    "                      \"OSD_gender\": fairness_part1_Gender['Overall score difference']['R2'],\n",
    "                      \"OSD_gender_p_value\": fairness_part1_Gender['Overall score difference']['sig'],\n",
    "                      \"CSD_gender\": fairness_part1_Gender['Conditional score difference']['R2'],\n",
    "                      \"CSD_gender_p_value\": fairness_part1_Gender['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_gender\":fairness_part2_Gender['SMD_1'][0],\n",
    "                      \"SMD_0_gender\":fairness_part2_Gender['SMD_0'][0],\n",
    "                      \"MAED_gender\":fairness_part2_Gender['diff_mae'][0],\n",
    "                      \"OSA_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score difference']['R2'],\n",
    "                      \"OSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score difference']['sig'],\n",
    "                      \"CSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Conditional score difference']['R2'],\n",
    "                      \"CSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_1'][0],\n",
    "                      \"SMD_0_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_0'][0],\n",
    "                      \"MAED_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['diff_mae'][0],\n",
    "                      \"OSA_Disability\": fairness_part1_Disability['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Disability_p_value\": fairness_part1_Disability['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Disability\": fairness_part1_Disability['Overall score difference']['R2'],\n",
    "                      \"OSD_Disability_p_value\": fairness_part1_Disability['Overall score difference']['sig'],\n",
    "                      \"CSD_Disability\": fairness_part1_Disability['Conditional score difference']['R2'],\n",
    "                      \"CSD_Disability_p_value\": fairness_part1_Disability['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Disability\":fairness_part2_Disability['SMD_1'][0],\n",
    "                      \"SMD_0_Disability\":fairness_part2_Disability['SMD_0'][0],\n",
    "                      \"MAED_Disability\":fairness_part2_Disability['diff_mae'][0],\n",
    "                      \"OSA_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score accuracy']['R2'],\n",
    "                      \"OSA_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score accuracy']['sig'],\n",
    "                      \"OSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score difference']['R2'],\n",
    "                      \"OSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score difference']['sig'],\n",
    "                      \"CSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Conditional score difference']['R2'],\n",
    "                      \"CSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_1'][0],\n",
    "                      \"SMD_0_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_0'][0],\n",
    "                      \"MAED_English_Language_Learner\":fairness_part2_English_Language_Learner['diff_mae'][0],\n",
    "                      \"OSA_Race\": fairness_part1_Race['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Race_p_value\": fairness_part1_Race['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Race\": fairness_part1_Race['Overall score difference']['R2'],\n",
    "                      \"OSD_Race_p_value\": fairness_part1_Race['Overall score difference']['sig'],\n",
    "                      \"CSD_Race\": fairness_part1_Race['Conditional score difference']['R2'],\n",
    "                      \"CSD_Race_p_value\": fairness_part1_Race['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Race\":fairness_part2_Race['SMD_1'][0],\n",
    "                      \"SMD_0_Race\":fairness_part2_Race['SMD_0'][0],\n",
    "                      \"MAED_Race\":fairness_part2_Race['diff_mae'][0]}\n",
    "      df = df.append(new_row, ignore_index=True)\n",
    "      k += 1\n",
    "    df.to_csv('', index=False)\n",
    "    i += 1\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1FsAXwsDNnq"
   },
   "outputs": [],
   "source": [
    "res = run_experiment(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6I-rHDr-mnY-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxps4Wew+eXfykNirtgEWO",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1qM3G9pmTfb779HCO7m7NmnnEJ69-9Loy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
