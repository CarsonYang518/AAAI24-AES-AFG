{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18206,
     "status": "ok",
     "timestamp": 1690275096732,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "KMVuj2RCQe1e",
    "outputId": "89a7542b-9d1b-446a-e78e-196494c51d99"
   },
   "outputs": [],
   "source": [
    "!pip install rsmtool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13952,
     "status": "ok",
     "timestamp": 1690275110682,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "VC9J6S1nQi2P",
    "outputId": "a00125e8-f320-4611-ec8c-7990391a0d52"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n",
    "from rsmtool.utils.metrics import quadratic_weighted_kappa, difference_of_standardized_means, standardized_mean_difference\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from rsmtool.fairness_utils import get_fairness_analyses\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1690275110682,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "dAsoAWy9RGuS"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS= 4000\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1690275110682,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "1j6vArSnQmos"
   },
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CustomLoss, self).__init__()\n",
    "\n",
    "  def forward(self, predictions, targets):\n",
    "    loss = torch.mean((predictions - targets) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1690275110683,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "S2O-trPOQyoJ"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  prompt_1 = pd.read_csv(path+'Prompt_1.csv')\n",
    "  prompt_2 = pd.read_csv(path+'Prompt_2.csv')\n",
    "  prompt_3 = pd.read_csv(path+'Prompt_3.csv')\n",
    "  prompt_4 = pd.read_csv(path+'Prompt_4.csv')\n",
    "  prompt_5 = pd.read_csv(path+'Prompt_5.csv')\n",
    "  prompt_6 = pd.read_csv(path+'Prompt_6.csv')\n",
    "  prompt_7 = pd.read_csv(path+'Prompt_7.csv')\n",
    "  prompt_8 = pd.read_csv(path+'Prompt_8.csv')\n",
    "  prompt_9 = pd.read_csv(path+'Prompt_9.csv')\n",
    "  prompt_10 = pd.read_csv(path+'Prompt_10.csv')\n",
    "  prompt_11 = pd.read_csv(path+'Prompt_11.csv')\n",
    "  prompt_12 = pd.read_csv(path+'Prompt_12.csv')\n",
    "  return [prompt_1, prompt_2, prompt_3, prompt_4, prompt_5, prompt_6, prompt_7, prompt_8, prompt_9, prompt_10, prompt_11, prompt_12]\n",
    "\n",
    "\n",
    "def split_data(data, fold):\n",
    "    kfold = KFold(n_splits=fold, shuffle=False)\n",
    "    results = []\n",
    "    for train_index, test_index in kfold.split(data):\n",
    "        results.append((train_index, test_index))\n",
    "    return results\n",
    "\n",
    "def accuracy_evaluation(y_pred, y_test):\n",
    "    qwk = quadratic_weighted_kappa(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    pearson_score = pearsonr(y_test, y_pred).statistic\n",
    "    return qwk, mae, pearson_score\n",
    "\n",
    "def fairness_evaluation(y_pred, y_test, demo_attribute):\n",
    "    df = pd.DataFrame({\"True_Score\":y_test, \"Prediction_Score\":y_pred, \"Demo\":demo_attribute})\n",
    "    results = get_fairness_analyses(df, group=\"Demo\", system_score_column=\"Prediction_Score\", human_score_column=\"True_Score\")[1].values()[3]\n",
    "    population_y_true_observed_sd = np.std(y_test)\n",
    "    population_y_true_observed_mn = np.mean(y_test)\n",
    "    population_y_pred_sd = np.std(y_pred)\n",
    "    population_y_pred_mn = np.mean(y_pred)\n",
    "    y_test_demo_0 = y_test[np.where(demo_attribute==0)]\n",
    "    y_test_demo_1 = y_test[np.where(demo_attribute==1)]\n",
    "    y_pred_demo_0 = y_pred[np.where(demo_attribute==0)]\n",
    "    y_pred_demo_1 = y_pred[np.where(demo_attribute==1)]\n",
    "    SMD_0 = difference_of_standardized_means(y_test_demo_0, y_pred_demo_0, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    SMD_1 = difference_of_standardized_means(y_test_demo_1, y_pred_demo_1, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    diff_mae = mean_absolute_error(y_test_demo_1, y_pred_demo_1) - mean_absolute_error(y_test_demo_0, y_pred_demo_0)\n",
    "    scores = pd.DataFrame({\"SMD_0\":[SMD_0], \"SMD_1\":[SMD_1], \"diff_mae\":[diff_mae]})\n",
    "    return results, scores\n",
    "\n",
    "def covert_label(y_pred):\n",
    "  range_min = 1\n",
    "  range_max = 6\n",
    "  y_orig = [(score*(range_max-range_min)+range_min) for score in y_pred]\n",
    "  return y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1690275110683,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "-_lN2egLQ0lr"
   },
   "outputs": [],
   "source": [
    "def get_features(texts, labels):\n",
    "  range_min = min(labels)\n",
    "  range_max = max(labels)\n",
    "  y = (labels-range_min)/(range_max-range_min)\n",
    "\n",
    "  maxSenNum = 0\n",
    "  maxSenLen = 0\n",
    "  documents = []\n",
    "  for text in texts:\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    documents.append(word_tokens)\n",
    "    if len(sentences) > maxSenNum:\n",
    "      maxSenNum = len(sentences)\n",
    "    if max([len(word_token) for word_token in word_tokens]) > maxSenLen:\n",
    "      maxSenLen = max([len(word_token) for word_token in word_tokens])\n",
    "\n",
    "  X = []\n",
    "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token = \"<unk>\")\n",
    "  tokenizer.fit_on_texts(texts)\n",
    "  word_index = tokenizer.word_index\n",
    "  for document in documents:\n",
    "    sequences = tokenizer.texts_to_sequences(document)\n",
    "    X.append(pad_sequences(sequences, maxlen=maxSenLen))\n",
    "  max_shape = np.array(max(X, key=lambda x: x.shape)).shape\n",
    "  padded_X = np.array([np.pad(array, ((0, max_shape[0]-array.shape[0]), (0, max_shape[1]-array.shape[1])), mode='constant', constant_values=0) for array in X])\n",
    "  return padded_X, y, word_index, maxSenNum, maxSenLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1690275112398,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "nccpGUB7RI1t"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(word_index):\n",
    "  fp1=open(\"\",\"r\")\n",
    "  glove_emb={}\n",
    "  for line in fp1:\n",
    "    temp=line.split(\" \")\n",
    "    glove_emb[temp[0]]=np.asarray([float(i) for i in temp[1:]])\n",
    "\n",
    "  embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "  for word,i in word_index.items():\n",
    "    if i>= MAX_NB_WORDS:\n",
    "      continue\n",
    "    if word in glove_emb:\n",
    "      embedding_matrix[i]=glove_emb[word]\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112398,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "QwO4c4ZYSC_T"
   },
   "outputs": [],
   "source": [
    "class WordAttNet(nn.Module):\n",
    "  def __init__(self, dict,hidden_size=100):\n",
    "    super(WordAttNet, self).__init__()\n",
    "    dict = torch.from_numpy(dict.astype(np.float32))\n",
    "    self.lookup = nn.Embedding(num_embeddings=MAX_NB_WORDS, embedding_dim=50).from_pretrained(dict)\n",
    "    self.conv1 = nn.Conv1d(in_channels=50,out_channels=100,kernel_size=5)\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "    self.fc1 = nn.Linear(100,100)\n",
    "    self.fc2 = nn.Linear(100,1,bias =False)\n",
    "\n",
    "  def forward(self, input):\n",
    "    output = self.lookup(input)\n",
    "    output = self.dropout(output)\n",
    "    output = output.permute(1,2,0)\n",
    "    f_output = self.conv1(output.float())\n",
    "    f_output = f_output.permute(2,0,1)\n",
    "\n",
    "    weight = torch.tanh(self.fc1(f_output))\n",
    "    weight = self.fc2(weight)\n",
    "    weight = F.softmax(weight,0)\n",
    "    weight = weight * f_output\n",
    "    output = weight.sum(0).unsqueeze(0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112398,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "njc-r6OSSHPt"
   },
   "outputs": [],
   "source": [
    "class SentAttNet(nn.Module):\n",
    "  def __init__(self, sent_hidden_size=100, word_hidden_size=100):\n",
    "    super(SentAttNet, self).__init__()\n",
    "    self.LSTM = nn.LSTM(word_hidden_size, sent_hidden_size)\n",
    "    self.fc = nn.Linear( sent_hidden_size, 1)\n",
    "    self.fc1 = nn.Linear( sent_hidden_size,sent_hidden_size)\n",
    "    self.fc2 = nn.Linear( sent_hidden_size , 1,bias =False)\n",
    "\n",
    "  def forward(self, input):\n",
    "    f_output, _ = self.LSTM(input)\n",
    "    weight = torch.tanh(self.fc1(f_output))\n",
    "    weight = self.fc2(weight)\n",
    "    weight = F.softmax(weight,0)\n",
    "    weight = weight * f_output\n",
    "    output = weight.sum(0)\n",
    "    output = torch.sigmoid(self.fc(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112399,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "uAmWhUslSJP2"
   },
   "outputs": [],
   "source": [
    "class HierAttNet(nn.Module):\n",
    "  def __init__(self, word_hidden_size, sent_hidden_size, batch_size, embed_table, max_sent_length, max_word_length):\n",
    "    super(HierAttNet, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.word_hidden_size = word_hidden_size\n",
    "    self.sent_hidden_size = sent_hidden_size\n",
    "    self.max_sent_length = max_sent_length\n",
    "    self.max_word_length = max_word_length\n",
    "    self.word_att_net = WordAttNet(embed_table, word_hidden_size)\n",
    "    self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size)\n",
    "    self._init_hidden_state()\n",
    "\n",
    "  def _init_hidden_state(self, last_batch_size=None):\n",
    "    if last_batch_size:\n",
    "      batch_size = last_batch_size\n",
    "    else:\n",
    "      batch_size = self.batch_size\n",
    "      self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
    "      self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
    "      self.word_hidden_state = self.word_hidden_state.cuda()\n",
    "      self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
    "\n",
    "  def forward(self, input):\n",
    "    output_list = torch.empty(0,).cuda()\n",
    "    input = input.permute(1, 0, 2)\n",
    "    for i in input:\n",
    "      output = self.word_att_net(i.permute(1, 0))\n",
    "      output_list = torch.cat((output_list,output))\n",
    "    output= self.sent_att_net(output_list)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112399,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "CsvjhFjWSLnu"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_dataloader, dev_dataloader, epochs):\n",
    "  best_qwk = float('-inf')\n",
    "  for i in range(0, epochs):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    print(\"Epoch \" + str(i+1))\n",
    "    for iter, (feature, label) in tqdm(enumerate(train_dataloader)):\n",
    "      model.zero_grad()\n",
    "      cuda_feature = feature.cuda()\n",
    "      cuda_labels = label.cuda()\n",
    "\n",
    "      outputs = model(cuda_feature)\n",
    "      loss = criterion(outputs, cuda_labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      total_loss += loss.item()\n",
    "      count += 1\n",
    "\n",
    "    qwk = evaluate(model, criterion, dev_dataloader)\n",
    "    if best_qwk < qwk:\n",
    "        best_qwk = qwk\n",
    "        torch.save(model.state_dict(), '')\n",
    "    print(\"Epoch {} complete, train loss: {}, dev qwk: {}\".format(i+1, total_loss/count, qwk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112399,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "GnT2A2aXS9nq"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dev_dataloader):\n",
    "  model.eval()\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "  with torch.no_grad():\n",
    "    for iter, (feature, label) in enumerate(dev_dataloader):\n",
    "      cuda_feature = feature.cuda()\n",
    "      cuda_labels = label.cuda()\n",
    "      outputs = model(cuda_feature)\n",
    "\n",
    "      results = outputs.squeeze(-1)\n",
    "      results = results.detach().cpu().numpy()\n",
    "      for result in results:\n",
    "        y_pred.append(result)\n",
    "\n",
    "      labels = cuda_labels.squeeze(-1)\n",
    "      labels = labels.detach().cpu().numpy()\n",
    "      for label in labels:\n",
    "        y_true.append(label)\n",
    "  y_true = covert_label(y_true)\n",
    "  y_pred = covert_label(y_pred)\n",
    "  qwk = quadratic_weighted_kappa(y_true, y_pred)\n",
    "  return qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1690275112399,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "YueO4ViyTEco"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "  model.eval()\n",
    "  y_pred = []\n",
    "  y_true = []\n",
    "  with torch.no_grad():\n",
    "    for iter, (feature, label) in enumerate(dataloader):\n",
    "      cuda_feature = feature.cuda()\n",
    "      cuda_labels = label.cuda()\n",
    "      outputs = model(cuda_feature)\n",
    "\n",
    "      results = outputs.squeeze(-1)\n",
    "      results = results.detach().cpu().numpy()\n",
    "      for result in results:\n",
    "        y_pred.append(result)\n",
    "\n",
    "      labels = cuda_labels.squeeze(-1)\n",
    "      labels = labels.detach().cpu().numpy()\n",
    "      for label in labels:\n",
    "        y_true.append(label)\n",
    "  y_true = covert_label(y_true)\n",
    "  y_pred = covert_label(y_pred)\n",
    "  return np.array(y_pred), np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1690275122137,
     "user": {
      "displayName": "KaiXun Yang",
      "userId": "01445483365957018279"
     },
     "user_tz": -600
    },
    "id": "MdhmNg7rT684"
   },
   "outputs": [],
   "source": [
    "def run_experiment(seed):\n",
    "  df = pd.DataFrame(columns=[\"prompt\", \"fold\", \"quadratic_weighted_kappa\", \"mean_absolute_error\", \"pearson_correlation_coefficient\",\n",
    "                            \"OSA_gender\", \"OSA_gender_p_value\", \"OSD_gender\", \"OSD_gender_p_value\", \"CSD_gender\", \"CSD_gender_p_value\", \"SMD_1_gender\", \"SMD_0_gender\", \"MAED_gender\",\n",
    "                            \"OSA_Economically_disadvantaged\", \"OSA_Economically_disadvantaged_p_value\", \"OSD_Economically_disadvantaged\", \"OSD_Economically_disadvantaged_p_value\", \"CSD_Economically_disadvantaged\", \"CSD_Economically_disadvantaged_p_value\", \"SMD_1_Economically_disadvantaged\", \"SMD_0_Economically_disadvantaged\", \"MAED_Economically_disadvantaged\",\n",
    "                            \"OSA_Disability\", \"OSA_Disability_p_value\", \"OSD_Disability\", \"OSD_Disability_p_value\", \"CSD_Disability\", \"CSD_Disability_p_value\", \"SMD_1_Disability\", \"SMD_0_Disability\", \"MAED_Disability\",\n",
    "                            \"OSA_English_Language_Learner\", \"OSA_English_Language_Learner_p_value\", \"OSD_English_Language_Learner\", \"OSD_English_Language_Learner_p_value\", \"CSD_English_Language_Learner\", \"CSD_English_Language_Learner_p_value\", \"SMD_1_English_Language_Learner\", \"SMD_0_English_Language_Learner\", \"MAED_English_Language_Learner\",\n",
    "                            \"OSA_Race\", \"OSA_Race_p_value\", \"OSD_Race\", \"OSD_Race_p_value\", \"CSD_Race\", \"CSD_Race_p_value\", \"SMD_1_Race\", \"SMD_0_Race\", \"MAED_Race\"])\n",
    "  gpu = 0\n",
    "  criterion = CustomLoss()\n",
    "  epochs = 50\n",
    "  prompts = load_data(\"\")\n",
    "  batch_size = 10\n",
    "  i = 0\n",
    "  for prompt in prompts:\n",
    "    print(\"Prompt\"+str(i+1))\n",
    "    kfolds = split_data(prompt, 5)\n",
    "    k = 0\n",
    "    for kfold in kfolds:\n",
    "      X, y, word_index, maxSenNum, maxSenLen = get_features(prompt['Text'], prompt['Overall'])\n",
    "      embed_table = get_embeddings(word_index)\n",
    "\n",
    "      X_train_all = X[kfold[0]]\n",
    "      y_train_all = y[kfold[0]]\n",
    "\n",
    "      X_test = X[kfold[1]]\n",
    "      y_test = y[kfold[1]]\n",
    "      test_info = prompt.iloc[kfold[1]]\n",
    "\n",
    "      X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.25, random_state=seed)\n",
    "\n",
    "      train_data = Data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train.to_numpy().reshape(-1, 1)))\n",
    "      val_data = Data.TensorDataset(torch.tensor(X_val), torch.tensor(y_val.to_numpy().reshape(-1, 1)))\n",
    "      test_data = Data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test.to_numpy().reshape(-1, 1)))\n",
    "\n",
    "      train_dataloader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "      dev_dataloader = Data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "      test_dataloader = Data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "      model = HierAttNet(100, 100, batch_size, embed_table, maxSenNum, maxSenLen)\n",
    "      model.cuda()\n",
    "      optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, alpha=0.9)\n",
    "      model.word_att_net.lookup.weight.requires_grad = True\n",
    "\n",
    "      train(model, optimizer, criterion, train_dataloader, dev_dataloader, epochs)\n",
    "\n",
    "      best_model = HierAttNet(100, 100, batch_size, embed_table, maxSenNum, maxSenLen)\n",
    "      best_model.cuda()\n",
    "      best_model.load_state_dict(torch.load(''))\n",
    "\n",
    "      y_pred, y_true = predict(best_model, test_dataloader)\n",
    "      qwk, mae, pearson_score = accuracy_evaluation(y_pred, y_true)\n",
    "      print(str(qwk), str(mae), str(pearson_score))\n",
    "      fairness_part1_Gender, fairness_part2_Gender = fairness_evaluation(y_pred, y_true, test_info['Gender'].to_numpy())\n",
    "      fairness_part1_Economically_disadvantaged, fairness_part2_Economically_disadvantaged = fairness_evaluation(y_pred, y_true, test_info['Economically_disadvantaged'].to_numpy())\n",
    "      fairness_part1_Disability, fairness_part2_Disability = fairness_evaluation(y_pred, y_true, test_info['Disability'].to_numpy())\n",
    "      fairness_part1_English_Language_Learner, fairness_part2_English_Language_Learner = fairness_evaluation(y_pred, y_true, test_info['English_Language_Learner'].to_numpy())\n",
    "      fairness_part1_Race, fairness_part2_Race = fairness_evaluation(y_pred, y_true, test_info['Race_Binary'].to_numpy())\n",
    "      new_row = {\"prompt\" : i+1, \"fold\": k+1, \"quadratic_weighted_kappa\": qwk, \"mean_absolute_error\": mae, \"pearson_correlation_coefficient\": pearson_score,\n",
    "                      \"OSA_gender\": fairness_part1_Gender['Overall score accuracy']['R2'],\n",
    "                      \"OSA_gender_p_value\": fairness_part1_Gender['Overall score accuracy']['sig'],\n",
    "                      \"OSD_gender\": fairness_part1_Gender['Overall score difference']['R2'],\n",
    "                      \"OSD_gender_p_value\": fairness_part1_Gender['Overall score difference']['sig'],\n",
    "                      \"CSD_gender\": fairness_part1_Gender['Conditional score difference']['R2'],\n",
    "                      \"CSD_gender_p_value\": fairness_part1_Gender['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_gender\":fairness_part2_Gender['SMD_1'][0],\n",
    "                      \"SMD_0_gender\":fairness_part2_Gender['SMD_0'][0],\n",
    "                      \"MAED_gender\":fairness_part2_Gender['diff_mae'][0],\n",
    "                      \"OSA_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score difference']['R2'],\n",
    "                      \"OSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score difference']['sig'],\n",
    "                      \"CSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Conditional score difference']['R2'],\n",
    "                      \"CSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_1'][0],\n",
    "                      \"SMD_0_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_0'][0],\n",
    "                      \"MAED_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['diff_mae'][0],\n",
    "                      \"OSA_Disability\": fairness_part1_Disability['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Disability_p_value\": fairness_part1_Disability['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Disability\": fairness_part1_Disability['Overall score difference']['R2'],\n",
    "                      \"OSD_Disability_p_value\": fairness_part1_Disability['Overall score difference']['sig'],\n",
    "                      \"CSD_Disability\": fairness_part1_Disability['Conditional score difference']['R2'],\n",
    "                      \"CSD_Disability_p_value\": fairness_part1_Disability['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Disability\":fairness_part2_Disability['SMD_1'][0],\n",
    "                      \"SMD_0_Disability\":fairness_part2_Disability['SMD_0'][0],\n",
    "                      \"MAED_Disability\":fairness_part2_Disability['diff_mae'][0],\n",
    "                      \"OSA_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score accuracy']['R2'],\n",
    "                      \"OSA_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score accuracy']['sig'],\n",
    "                      \"OSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score difference']['R2'],\n",
    "                      \"OSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score difference']['sig'],\n",
    "                      \"CSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Conditional score difference']['R2'],\n",
    "                      \"CSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_1'][0],\n",
    "                      \"SMD_0_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_0'][0],\n",
    "                      \"MAED_English_Language_Learner\":fairness_part2_English_Language_Learner['diff_mae'][0],\n",
    "                      \"OSA_Race\": fairness_part1_Race['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Race_p_value\": fairness_part1_Race['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Race\": fairness_part1_Race['Overall score difference']['R2'],\n",
    "                      \"OSD_Race_p_value\": fairness_part1_Race['Overall score difference']['sig'],\n",
    "                      \"CSD_Race\": fairness_part1_Race['Conditional score difference']['R2'],\n",
    "                      \"CSD_Race_p_value\": fairness_part1_Race['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Race\":fairness_part2_Race['SMD_1'][0],\n",
    "                      \"SMD_0_Race\":fairness_part2_Race['SMD_0'][0],\n",
    "                      \"MAED_Race\":fairness_part2_Race['diff_mae'][0]}\n",
    "      df = df.append(new_row, ignore_index=True)\n",
    "      k += 1\n",
    "    df.to_csv('', index=False)\n",
    "    i += 1\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpxBcgljV9oK"
   },
   "outputs": [],
   "source": [
    "run_experiment(0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNHZT2Q9jmLWAGteN+hp++V",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1TdyJk07wlLUe5Y1oqTNEJhmZqUo0i5p0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
