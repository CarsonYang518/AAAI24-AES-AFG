{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import Tree\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tree.prettyprinter import TreePrettyPrinter\n",
    "import stanza\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import lftk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import ngrams\n",
    "from language_tool_python import LanguageTool\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd34a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    prompt_1 = pd.read_csv(path+'prompt_1.csv')\n",
    "    prompt_2 = pd.read_csv(path+'prompt_2.csv')\n",
    "    prompt_3 = pd.read_csv(path+'prompt_3.csv')\n",
    "    prompt_4 = pd.read_csv(path+'prompt_4.csv')\n",
    "    prompt_5 = pd.read_csv(path+'prompt_5.csv')\n",
    "    prompt_6 = pd.read_csv(path+'prompt_6.csv')\n",
    "    prompt_7 = pd.read_csv(path+'prompt_7.csv')\n",
    "    prompt_8 = pd.read_csv(path+'prompt_8.csv')\n",
    "    prompt_9 = pd.read_csv(path+'prompt_9.csv')\n",
    "    prompt_10 = pd.read_csv(path+'prompt_10.csv')\n",
    "    prompt_11 = pd.read_csv(path+'prompt_11.csv')\n",
    "    prompt_12 = pd.read_csv(path+'prompt_12.csv')\n",
    "    return prompt_1, prompt_2, prompt_3, prompt_4, prompt_5, prompt_6, prompt_7, prompt_8, prompt_9, prompt_10, prompt_11, prompt_12\n",
    "\n",
    "def tree_height(root):\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)\n",
    "\n",
    "# avg length of words, avg length of sentences, type-token-ratio, formality\n",
    "def get_features_one(data):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs = []\n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        doc = nlp(text)\n",
    "        docs.append(doc)\n",
    "    LFTK = lftk.Extractor(docs=docs)\n",
    "    extracted_features = LFTK.extract(features = ['t_word', 't_sent', 'a_word_ps', 'a_char_pw', 'simp_ttr', 'n_noun', 'n_adj', 'n_pron', 'n_det', 'n_adp', 'n_verb', 'n_adv', 'n_intj'])\n",
    "    features = pd.DataFrame(extracted_features)\n",
    "    features['formality'] = ((features['n_noun']+features['n_adj']+features['n_adj']+features['n_det'])/features['t_word'] - (features['n_adp']+features['n_verb']+features['n_adv']+features['n_intj'])/features['t_word'] + 100)/2\n",
    "    return features.drop(columns=['n_noun', 'n_adj', 'n_pron', 'n_det', 'n_adp', 'n_verb', 'n_adv', 'n_intj', 't_word', 't_sent'])\n",
    "\n",
    "# # of commas, # of apostrophe, # of period marks, # of exclamation marks, # of question marks\n",
    "def get_features_two(data):\n",
    "    result = []\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        tokenizer = RegexpTokenizer(punctuation_pattern)\n",
    "        punctuation_symbols = tokenizer.tokenize(text)\n",
    "        comma_count = punctuation_symbols.count(',')\n",
    "        apostrophe_count = punctuation_symbols.count(\"'\")\n",
    "        period_count = punctuation_symbols.count(\".\")\n",
    "        exclamation_count = punctuation_symbols.count(\"!\")\n",
    "        question_count = punctuation_symbols.count(\"?\")\n",
    "        result.append([comma_count, apostrophe_count, period_count, exclamation_count, question_count])\n",
    "    return pd.DataFrame(result, columns=['n_comma', 'n_apostrophe', 'n_period', 'n_exclamation', 'n_question'])\n",
    "\n",
    "\n",
    "def get_features_three(data):\n",
    "    result = []\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency', use_gpu=True, pos_batch_size=3000)\n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        doc = nlp(text)\n",
    "        tree_heights = [tree_height(sentence.constituency) for sentence in doc.sentences]\n",
    "        avg_tree_height = sum(tree_heights) / len(tree_heights)\n",
    "        num_NPs = [len(re.findall(\"NP\", str(sentence.constituency))) for sentence in doc.sentences]\n",
    "        avg_num_NPs = sum(num_NPs) / len(num_NPs)\n",
    "        num_VPs = [len(re.findall(\"VP\", str(sentence.constituency))) for sentence in doc.sentences]\n",
    "        avg_num_VPs = sum(num_VPs) / len(num_VPs)\n",
    "        num_SBARs = [len(re.findall(\"SBAR\", str(sentence.constituency))) for sentence in doc.sentences]\n",
    "        avg_num_SBARs = sum(num_SBARs) / len(num_SBARs)\n",
    "        result.append([avg_tree_height, avg_num_NPs, avg_num_VPs, avg_num_SBARs])\n",
    "    return pd.DataFrame(result, columns=['avg_tree_height', 'avg_num_NPs', 'avg_num_VPs', 'avg_num_SBARs'])\n",
    "\n",
    "# average word frequency in brown corpus, cohesion\n",
    "def get_features_four(data):\n",
    "    connectives_list = [\n",
    "        'accordingly', 'conversely', 'to the right', 'soon', 'presently', 'after', 'also',\n",
    "        'because of this', 'gradually', 'hence', 'to the left', 'then', 'but', 'and', 'of equal importance',\n",
    "        'afterward', 'still', 'briefly', 'on the other side', 'across the hall', 'adjacent to',\n",
    "        'as soon as', 'for this purpose', 'yet', 'and yet', 'in spite of this', 'as a result',\n",
    "        'as a consequence', 'here', 'at last', 'directly ahead', 'before', 'beyond', 'on the other hand',\n",
    "        'to repeat', 'at length', 'in the same way', 'such as', 'the next months', 'with this in mind',\n",
    "        'below', 'just as important', 'as you turn right', 'in short', 'second', 'when', 'last of all',\n",
    "        'in contrast', 'equally important', 'subsequently', 'consequently', 'from here on', 'furthermore',\n",
    "        'thus', 'on the following day', 'next', 'ultimately', 'as you can see', 'further', 'behind',\n",
    "        'besides', 'to be specific', 'finally', 'on the whole', 'to illustrate', 'in the meantime',\n",
    "        'nearby', 'similarly', 'as I have said', 'nonetheless', 'at this point', 'to this end',\n",
    "        'in the end', 'at the top', 'in addition', 'for example', 'in the background', 'thereafter',\n",
    "        'the next week', 'lastly', 'for instance', 'or', 'in conclusion', 'after a short time', 'like',\n",
    "        'the next day', 'since', 'along the wall', 'first', 'there', 'nevertheless', 'too', 'opposite',\n",
    "        'above', 'as so', 'moreover', 'in fact', 'in the same manner', 'last', 'therefore', 'on the contrary',\n",
    "        'however', 'so', 'now', 'to begin with', 'another', 'a minute later', 'meanwhile', 'to sum up',\n",
    "        'actually', 'for this reason', 'later', 'in summary'\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    stemmer = PorterStemmer()\n",
    "    word_counts = Counter()\n",
    "    for category in brown.categories():\n",
    "        for word in brown.words(categories=category):\n",
    "            word = word.lower()\n",
    "            word = stemmer.stem(word)\n",
    "            word_counts[word] += 1\n",
    "    \n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        words = word_tokenize(text)\n",
    "        total_frequency = 0\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = stemmer.stem(word)\n",
    "            if word in word_counts.keys():\n",
    "                total_frequency += word_counts[word]\n",
    "                \n",
    "        total_connectives = 0\n",
    "        for connective in connectives_list:\n",
    "            total_connectives += len(connective.split(' ')) * len(re.findall(r'\\b{}\\b'.format(re.escape(connective)), text.lower()))\n",
    "            \n",
    "        cohesion = total_connectives/len(words)\n",
    "        frequency = total_frequency/len(words)\n",
    "        result.append([frequency, cohesion])\n",
    "        \n",
    "    return pd.DataFrame(result, columns=['frequency', 'cohesion'])\n",
    "\n",
    "# n-gram overlap coherence, redundancy of nouns coherence\n",
    "def get_features_five(data):\n",
    "    result = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        sentences = sent_tokenize(text.lower())\n",
    "        overlaps = []\n",
    "        nouns_overlaps = []\n",
    "        for i in range(len(sentences)-1):\n",
    "            redundancy = 0.0\n",
    "            nouns_redundancy = 0.0\n",
    "            \n",
    "            tokens1 = [lemmatizer.lemmatize(word) for word in word_tokenize(sentences[i])]\n",
    "            tokens2 = [lemmatizer.lemmatize(word) for word in word_tokenize(sentences[i+1])]\n",
    "            tagged1 = pos_tag(tokens1)\n",
    "            tagged2 = pos_tag(tokens2)\n",
    "            nouns1 = [token for token, pos in tagged1 if pos.startswith(\"NN\")]\n",
    "            nouns2 = [token for token, pos in tagged2 if pos.startswith(\"NN\")]\n",
    "            \n",
    "            nouns_overlap = set(nouns1) & set(nouns2)\n",
    "            total_nouns = set(nouns1) | set(nouns2)\n",
    "            \n",
    "            if len(total_nouns) > 0:\n",
    "                nouns_redundancy = len(nouns_overlap)/len(total_nouns)\n",
    "                \n",
    "            unigrams1 = list(ngrams(tokens1, 1))\n",
    "            unigrams2 = list(ngrams(tokens2, 1))\n",
    "            bigrams1 = list(ngrams(tokens1, 2))\n",
    "            bigrams2 = list(ngrams(tokens2, 2))\n",
    "            trigrams1 = list(ngrams(tokens1, 3))\n",
    "            trigrams2 = list(ngrams(tokens2, 3))\n",
    "            \n",
    "            overlap_count_1 = sum((Counter(unigrams1) & Counter(unigrams2)).values())\n",
    "            overlap_count_2 = sum((Counter(bigrams1) & Counter(bigrams2)).values())\n",
    "            overlap_count_3 = sum((Counter(trigrams1) & Counter(trigrams2)).values())\n",
    "            \n",
    "            total_ngram_1 = len(unigrams1) + len(unigrams2)\n",
    "            total_ngram_2 = len(unigrams1) + len(unigrams2)\n",
    "            total_ngram_3 = len(unigrams1) + len(unigrams2)\n",
    "            \n",
    "            if total_ngram_1>0:\n",
    "                redundancy += overlap_count_1 / total_ngram_1\n",
    "            if total_ngram_2>0:\n",
    "                redundancy += overlap_count_2 / total_ngram_2\n",
    "            if total_ngram_3>0:\n",
    "                redundancy += overlap_count_3 / total_ngram_3\n",
    "                \n",
    "            overlaps.append(redundancy)\n",
    "            nouns_overlaps.append(nouns_redundancy)\n",
    "        ngram_overlap_score = 0.0\n",
    "        nouns_overlap_score = 0.0\n",
    "        if len(overlaps) > 0:\n",
    "            ngram_overlap_score = sum(overlaps)/len(overlaps)\n",
    "        if len(nouns_overlaps) > 0:\n",
    "            nouns_overlap_score = sum(nouns_overlaps)/len(nouns_overlaps)\n",
    "        result.append([ngram_overlap_score, nouns_overlap_score])\n",
    "    return pd.DataFrame(result, columns=['ngram_overlap_score', 'nouns_overlap_score'])\n",
    "\n",
    "# # of errors, readability (Flesch, Coleman-Liau, ARI, Kincaid, FOG, Lix, and SMOG), corpus similarity\n",
    "def get_features_six(data):\n",
    "    result = []\n",
    "    tool = LanguageTool('en-US')\n",
    "    brown_words = brown.words()\n",
    "    brown_freq_dist = FreqDist(brown_words)\n",
    "    total_brown_words = len(brown_words)\n",
    "    brown_prob_dist = {word: count / total_brown_words for word, count in brown_freq_dist.items()}\n",
    "    \n",
    "    for text in tqdm(data['Text'].tolist()):\n",
    "        errors = tool.check(text)\n",
    "        flesch = textstat.flesch_reading_ease(text)\n",
    "        coleman_liau = textstat.coleman_liau_index(text)\n",
    "        ari = textstat.automated_readability_index(text)\n",
    "        kincaid = textstat.flesch_kincaid_grade(text)\n",
    "        fog = textstat.gunning_fog(text)\n",
    "        lix = textstat.lix(text)\n",
    "        smog = textstat.smog_index(text)\n",
    "\n",
    "        document_words = nltk.word_tokenize(text)\n",
    "        document_freq_dist = FreqDist(document_words)\n",
    "        total_document_words = len(document_words)\n",
    "        document_prob_dist = {word: count / total_document_words for word, count in document_freq_dist.items()}\n",
    "        kl_divergence = sum([document_prob_dist[word] * np.log2(document_prob_dist[word] / brown_prob_dist.get(word, 1e-10)) for word in document_prob_dist])\n",
    "        result.append([len(errors), flesch, coleman_liau, ari, kincaid, fog, lix, smog, kl_divergence])\n",
    "    return pd.DataFrame(result, columns=['num_errors', 'flesch_reading_ease', 'coleman_liau_index', 'automated_readability_index', 'flesch_kincaid_grade', 'gunning_fog', 'lix', 'smog_index', 'corpus_similarity'])\n",
    "\n",
    "def generate_independent_features(data):\n",
    "    features_independent = pd.concat([get_features_one(data), get_features_two(data), get_features_three(data), get_features_four(data), get_features_five(data), get_features_six(data)], axis=1)\n",
    "    return features_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc074c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1, prompt_2, prompt_3, prompt_4, prompt_5, prompt_6, prompt_7, prompt_8, prompt_9, prompt_10, prompt_11, prompt_12 = load_data(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56312a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt_1, prompt_2, prompt_3, prompt_4, prompt_5, prompt_6, prompt_7, prompt_8, prompt_9, prompt_10, prompt_11, prompt_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for prompt in prompts:\n",
    "    features_independent = generate_independent_features(prompt)\n",
    "    features_independent.to_csv('prompt_'+str(counter)+'_features_independent.csv', index=False)\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
