{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from rsmtool.utils.metrics import quadratic_weighted_kappa, difference_of_standardized_means, standardized_mean_difference\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from rsmtool.fairness_utils import get_fairness_analyses\n",
    "from sklearn.model_selection import KFold\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy\n",
    "import lftk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a092374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    prompt_1 = pd.read_csv(path+'Prompt_1.csv')\n",
    "    prompt_2 = pd.read_csv(path+'Prompt_2.csv')\n",
    "    prompt_3 = pd.read_csv(path+'Prompt_3.csv')\n",
    "    prompt_4 = pd.read_csv(path+'Prompt_4.csv')\n",
    "    prompt_5 = pd.read_csv(path+'Prompt_5.csv')\n",
    "    prompt_6 = pd.read_csv(path+'Prompt_6.csv')\n",
    "    prompt_7 = pd.read_csv(path+'Prompt_7.csv')\n",
    "    prompt_8 = pd.read_csv(path+'Prompt_8.csv')\n",
    "    prompt_9 = pd.read_csv(path+'Prompt_9.csv')\n",
    "    prompt_10 = pd.read_csv(path+'Prompt_10.csv')\n",
    "    prompt_11 = pd.read_csv(path+'Prompt_11.csv')\n",
    "    prompt_12 = pd.read_csv(path+'Prompt_12.csv')\n",
    "    prompt_1_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_1_features_independent.csv')\n",
    "    prompt_2_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_2_features_independent.csv')\n",
    "    prompt_3_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_3_features_independent.csv')\n",
    "    prompt_4_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_4_features_independent.csv')\n",
    "    prompt_5_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_5_features_independent.csv')\n",
    "    prompt_6_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_6_features_independent.csv')\n",
    "    prompt_7_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_7_features_independent.csv')\n",
    "    prompt_8_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_8_features_independent.csv')\n",
    "    prompt_9_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_9_features_independent.csv')\n",
    "    prompt_10_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_10_features_independent.csv')\n",
    "    prompt_11_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_11_features_independent.csv')\n",
    "    prompt_12_features_independent = pd.read_csv(path+'Task-Independent Features for Automated Essay Grading/prompt_12_features_independent.csv')\n",
    "    return [(prompt_1, prompt_1_features_independent), (prompt_2, prompt_2_features_independent), (prompt_3, prompt_3_features_independent), (prompt_4, prompt_4_features_independent), (prompt_5, prompt_5_features_independent), (prompt_6, prompt_6_features_independent),\n",
    "          (prompt_7, prompt_7_features_independent), (prompt_8, prompt_8_features_independent), (prompt_9, prompt_9_features_independent), (prompt_10, prompt_10_features_independent), (prompt_11, prompt_11_features_independent), (prompt_12, prompt_12_features_independent)]\n",
    "\n",
    "def accuracy_evaluation(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    qwk = quadratic_weighted_kappa(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    pearson_score = pearsonr(y_test, y_pred).statistic\n",
    "    return qwk, mae, pearson_score\n",
    "\n",
    "def fairness_evaluation(model, X_test, y_test, demo_attribute):\n",
    "    y_pred = model.predict(X_test)\n",
    "    df = pd.DataFrame({\"True_Score\":y_test, \"Prediction_Score\":y_pred, \"Demo\":demo_attribute})\n",
    "    results = get_fairness_analyses(df, group=\"Demo\", system_score_column=\"Prediction_Score\", human_score_column=\"True_Score\")[1].values()[3]\n",
    "    population_y_true_observed_sd = np.std(y_test)\n",
    "    population_y_true_observed_mn = np.mean(y_test)\n",
    "    population_y_pred_sd = np.std(y_pred)\n",
    "    population_y_pred_mn = np.mean(y_pred)\n",
    "    y_test_demo_0 = y_test[np.where(demo_attribute==0)]\n",
    "    y_test_demo_1 = y_test[np.where(demo_attribute==1)]\n",
    "    y_pred_demo_0 = y_pred[np.where(demo_attribute==0)]\n",
    "    y_pred_demo_1 = y_pred[np.where(demo_attribute==1)]\n",
    "    SMD_0 = difference_of_standardized_means(y_test_demo_0, y_pred_demo_0, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    SMD_1 = difference_of_standardized_means(y_test_demo_1, y_pred_demo_1, population_y_true_observed_mn, population_y_pred_mn, population_y_true_observed_sd, population_y_pred_sd)\n",
    "    diff_mae = mean_absolute_error(y_test_demo_1, y_pred_demo_1) - mean_absolute_error(y_test_demo_0, y_pred_demo_0)\n",
    "    scores = pd.DataFrame({\"SMD_0\":[SMD_0], \"SMD_1\":[SMD_1], \"diff_mae\":[diff_mae]})\n",
    "    return results, scores\n",
    "\n",
    "def split_data(data, fold):\n",
    "    kfold = KFold(n_splits=fold, shuffle=False)\n",
    "    results = []\n",
    "    for train_index, test_index in kfold.split(data):\n",
    "        results.append((train_index, test_index))\n",
    "    return results\n",
    "\n",
    "def generate_feature(data):\n",
    "    sentence_result = []\n",
    "    word_result = []\n",
    "    for text in data['Text'].tolist():\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        num_sentences = len(sentences)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        num_words = len(words)\n",
    "        sentence_result.append(num_sentences)\n",
    "        word_result.append(num_words)\n",
    "    features = pd.DataFrame({\"num_words\":word_result, \"num_sentences\": sentence_result})\n",
    "    return features\n",
    "\n",
    "def generate_word_n_gram(train, test):\n",
    "    ngram_freq = Counter()\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for text in train['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        unigrams_list = list(ngrams(tokens, 1))\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        trigrams_list = list(ngrams(tokens, 3))\n",
    "        ngram_freq.update(unigrams_list)\n",
    "        ngram_freq.update(bigrams_list)\n",
    "        ngram_freq.update(trigrams_list)\n",
    "    \n",
    "    top_ngrams = dict(ngram_freq.most_common(1000))\n",
    "    \n",
    "    train_result = []\n",
    "    test_result = []\n",
    "    \n",
    "    for text in train['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        text_ngram_freq = Counter()\n",
    "        unigrams_list = list(ngrams(tokens, 1))\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        trigrams_list = list(ngrams(tokens, 3))\n",
    "        text_ngram_freq.update(unigrams_list)\n",
    "        text_ngram_freq.update(bigrams_list)\n",
    "        text_ngram_freq.update(trigrams_list)\n",
    "        text_features = [text_ngram_freq[ngrams] if ngrams in text_ngram_freq.keys() else 0 for ngrams in top_ngrams.keys()]\n",
    "        train_result.append(text_features)\n",
    "    \n",
    "    for text in test['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        text_ngram_freq = Counter()\n",
    "        unigrams_list = list(ngrams(tokens, 1))\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        trigrams_list = list(ngrams(tokens, 3))\n",
    "        text_ngram_freq.update(unigrams_list)\n",
    "        text_ngram_freq.update(bigrams_list)\n",
    "        text_ngram_freq.update(trigrams_list)\n",
    "        text_features = [text_ngram_freq[ngrams] if ngrams in text_ngram_freq.keys() else 0 for ngrams in top_ngrams.keys()]\n",
    "        test_result.append(text_features)\n",
    "    \n",
    "    column_name = ['ngram_'+ str(i+1) for i in range(1000)]\n",
    "    return pd.DataFrame(train_result, columns=column_name), pd.DataFrame(test_result, columns=column_name)\n",
    "\n",
    "def generate_pos_n_gram(train, test):\n",
    "    pos_ngram_freq = Counter()\n",
    "    for text in train['Text'].tolist():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        tags = [tag[1] for tag in pos_tags]\n",
    "        unigrams_list = list(ngrams(tags, 1))\n",
    "        bigrams_list = list(ngrams(tags, 2))\n",
    "        trigrams_list = list(ngrams(tags, 3))\n",
    "        pos_ngram_freq.update(unigrams_list)\n",
    "        pos_ngram_freq.update(bigrams_list)\n",
    "        pos_ngram_freq.update(trigrams_list)\n",
    "\n",
    "    top_pos_ngrams = dict(pos_ngram_freq.most_common(1000))\n",
    "    \n",
    "    train_result = []\n",
    "    test_result = []\n",
    "    \n",
    "    for text in train['Text'].tolist():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        tags = [tag[1] for tag in pos_tags]\n",
    "        text_pos_ngram_freq = Counter()\n",
    "        unigrams_list = list(ngrams(tags, 1))\n",
    "        bigrams_list = list(ngrams(tags, 2))\n",
    "        trigrams_list = list(ngrams(tags, 3))\n",
    "        text_pos_ngram_freq.update(unigrams_list)\n",
    "        text_pos_ngram_freq.update(bigrams_list)\n",
    "        text_pos_ngram_freq.update(trigrams_list)\n",
    "        text_features = [text_pos_ngram_freq[ngrams] if ngrams in text_pos_ngram_freq.keys() else 0 for ngrams in top_pos_ngrams.keys()]\n",
    "        train_result.append(text_features)\n",
    "        \n",
    "    for text in test['Text'].tolist():\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        tags = [tag[1] for tag in pos_tags]\n",
    "        text_pos_ngram_freq = Counter()\n",
    "        unigrams_list = list(ngrams(tags, 1))\n",
    "        bigrams_list = list(ngrams(tags, 2))\n",
    "        trigrams_list = list(ngrams(tags, 3))\n",
    "        text_pos_ngram_freq.update(unigrams_list)\n",
    "        text_pos_ngram_freq.update(bigrams_list)\n",
    "        text_pos_ngram_freq.update(trigrams_list)\n",
    "        text_features = [text_pos_ngram_freq[ngrams] if ngrams in text_pos_ngram_freq.keys() else 0 for ngrams in top_pos_ngrams.keys()]\n",
    "        test_result.append(text_features)\n",
    "    \n",
    "    column_name = ['pos_ngram_'+ str(i+1) for i in range(1000)]\n",
    "    return pd.DataFrame(train_result, columns=column_name), pd.DataFrame(test_result, columns=column_name)\n",
    "\n",
    "def generate_partition_word_ngram(train, test):\n",
    "    ngram_freq_1 = Counter()\n",
    "    ngram_freq_2 = Counter()\n",
    "    ngram_freq_3 = Counter()\n",
    "    ngram_freq_4 = Counter()\n",
    "    ngram_freq_5 = Counter()\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    for text in train['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        tokens_part_size = len(tokens) // 5\n",
    "        \n",
    "        tokens_1 = tokens[:tokens_part_size]\n",
    "        tokens_2 = tokens[tokens_part_size:tokens_part_size*2]\n",
    "        tokens_3 = tokens[tokens_part_size*2:tokens_part_size*3]\n",
    "        tokens_4 = tokens[tokens_part_size*3:tokens_part_size*4]\n",
    "        tokens_5 = tokens[tokens_part_size*4:]\n",
    "        \n",
    "        unigrams_list_1 = list(ngrams(tokens_1, 1))\n",
    "        bigrams_list_1 = list(ngrams(tokens_1, 2))\n",
    "        trigrams_list_1 = list(ngrams(tokens_1, 3))\n",
    "        ngram_freq_1.update(unigrams_list_1)\n",
    "        ngram_freq_1.update(bigrams_list_1)\n",
    "        ngram_freq_1.update(trigrams_list_1)\n",
    "        \n",
    "        unigrams_list_2 = list(ngrams(tokens_2, 1))\n",
    "        bigrams_list_2 = list(ngrams(tokens_2, 2))\n",
    "        trigrams_list_2 = list(ngrams(tokens_2, 3))\n",
    "        ngram_freq_2.update(unigrams_list_2)\n",
    "        ngram_freq_2.update(bigrams_list_2)\n",
    "        ngram_freq_2.update(trigrams_list_2)\n",
    "        \n",
    "        unigrams_list_3 = list(ngrams(tokens_3, 1))\n",
    "        bigrams_list_3 = list(ngrams(tokens_3, 2))\n",
    "        trigrams_list_3 = list(ngrams(tokens_3, 3))\n",
    "        ngram_freq_3.update(unigrams_list_3)\n",
    "        ngram_freq_3.update(bigrams_list_3)\n",
    "        ngram_freq_3.update(trigrams_list_3)\n",
    "        \n",
    "        unigrams_list_4 = list(ngrams(tokens_4, 1))\n",
    "        bigrams_list_4 = list(ngrams(tokens_4, 2))\n",
    "        trigrams_list_4 = list(ngrams(tokens_4, 3))\n",
    "        ngram_freq_4.update(unigrams_list_4)\n",
    "        ngram_freq_4.update(bigrams_list_4)\n",
    "        ngram_freq_4.update(trigrams_list_4)\n",
    "        \n",
    "        unigrams_list_5 = list(ngrams(tokens_5, 1))\n",
    "        bigrams_list_5 = list(ngrams(tokens_5, 2))\n",
    "        trigrams_list_5 = list(ngrams(tokens_5, 3))\n",
    "        ngram_freq_5.update(unigrams_list_5)\n",
    "        ngram_freq_5.update(bigrams_list_5)\n",
    "        ngram_freq_5.update(trigrams_list_5)\n",
    "          \n",
    "    top_ngrams_1 = dict(ngram_freq_1.most_common(1000))\n",
    "    top_ngrams_2 = dict(ngram_freq_2.most_common(1000))\n",
    "    top_ngrams_3 = dict(ngram_freq_3.most_common(1000))\n",
    "    top_ngrams_4 = dict(ngram_freq_4.most_common(1000))\n",
    "    top_ngrams_5 = dict(ngram_freq_5.most_common(1000))\n",
    "    \n",
    "    train_result = []\n",
    "    for text in train['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        tokens_part_size = len(tokens) // 5\n",
    "    \n",
    "        tokens_1 = tokens[:tokens_part_size]\n",
    "        tokens_2 = tokens[tokens_part_size:tokens_part_size*2]\n",
    "        tokens_3 = tokens[tokens_part_size*2:tokens_part_size*3]\n",
    "        tokens_4 = tokens[tokens_part_size*3:tokens_part_size*4]\n",
    "        tokens_5 = tokens[tokens_part_size*4:]\n",
    "        \n",
    "        text_ngram_freq_1 = Counter()\n",
    "        unigrams_list_1 = list(ngrams(tokens_1, 1))\n",
    "        bigrams_list_1 = list(ngrams(tokens_1, 2))\n",
    "        trigrams_list_1 = list(ngrams(tokens_1, 3))\n",
    "        text_ngram_freq_1.update(unigrams_list_1)\n",
    "        text_ngram_freq_1.update(bigrams_list_1)\n",
    "        text_ngram_freq_1.update(trigrams_list_1)\n",
    "        text_features = [text_ngram_freq_1[ngrams] if ngrams in text_ngram_freq_1.keys() else 0 for ngrams in top_ngrams_1.keys()]\n",
    "        \n",
    "        text_ngram_freq_2 = Counter()\n",
    "        unigrams_list_2 = list(ngrams(tokens_2, 1))\n",
    "        bigrams_list_2 = list(ngrams(tokens_2, 2))\n",
    "        trigrams_list_2 = list(ngrams(tokens_2, 3))\n",
    "        text_ngram_freq_2.update(unigrams_list_2)\n",
    "        text_ngram_freq_2.update(bigrams_list_2)\n",
    "        text_ngram_freq_2.update(trigrams_list_2)\n",
    "        text_features.extend([text_ngram_freq_2[ngrams] if ngrams in text_ngram_freq_2.keys() else 0 for ngrams in top_ngrams_2.keys()])\n",
    "        \n",
    "        text_ngram_freq_3 = Counter()\n",
    "        unigrams_list_3 = list(ngrams(tokens_3, 1))\n",
    "        bigrams_list_3 = list(ngrams(tokens_3, 2))\n",
    "        trigrams_list_3 = list(ngrams(tokens_3, 3))\n",
    "        text_ngram_freq_3.update(unigrams_list_3)\n",
    "        text_ngram_freq_3.update(bigrams_list_3)\n",
    "        text_ngram_freq_3.update(trigrams_list_3)\n",
    "        text_features.extend([text_ngram_freq_3[ngrams] if ngrams in text_ngram_freq_3.keys() else 0 for ngrams in top_ngrams_3.keys()])\n",
    "        \n",
    "        text_ngram_freq_4 = Counter()\n",
    "        unigrams_list_4 = list(ngrams(tokens_4, 1))\n",
    "        bigrams_list_4 = list(ngrams(tokens_4, 2))\n",
    "        trigrams_list_4 = list(ngrams(tokens_4, 3))\n",
    "        text_ngram_freq_4.update(unigrams_list_4)\n",
    "        text_ngram_freq_4.update(bigrams_list_4)\n",
    "        text_ngram_freq_4.update(trigrams_list_4)\n",
    "        text_features.extend([text_ngram_freq_4[ngrams] if ngrams in text_ngram_freq_4.keys() else 0 for ngrams in top_ngrams_4.keys()])\n",
    "\n",
    "        text_ngram_freq_5 = Counter()\n",
    "        unigrams_list_5 = list(ngrams(tokens_5, 1))\n",
    "        bigrams_list_5 = list(ngrams(tokens_5, 2))\n",
    "        trigrams_list_5 = list(ngrams(tokens_5, 3))\n",
    "        text_ngram_freq_5.update(unigrams_list_5)\n",
    "        text_ngram_freq_5.update(bigrams_list_5)\n",
    "        text_ngram_freq_5.update(trigrams_list_5)\n",
    "        text_features.extend([text_ngram_freq_5[ngrams] if ngrams in text_ngram_freq_5.keys() else 0 for ngrams in top_ngrams_5.keys()])\n",
    "        \n",
    "        train_result.append(text_features)\n",
    "\n",
    "    test_result = []\n",
    "    for text in test['Text'].tolist():\n",
    "        text = text.translate(translator)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "        tokens_part_size = len(tokens) // 5\n",
    "    \n",
    "        tokens_1 = tokens[:tokens_part_size]\n",
    "        tokens_2 = tokens[tokens_part_size:tokens_part_size*2]\n",
    "        tokens_3 = tokens[tokens_part_size*2:tokens_part_size*3]\n",
    "        tokens_4 = tokens[tokens_part_size*3:tokens_part_size*4]\n",
    "        tokens_5 = tokens[tokens_part_size*4:]\n",
    "        \n",
    "        text_ngram_freq_1 = Counter()\n",
    "        unigrams_list_1 = list(ngrams(tokens_1, 1))\n",
    "        bigrams_list_1 = list(ngrams(tokens_1, 2))\n",
    "        trigrams_list_1 = list(ngrams(tokens_1, 3))\n",
    "        text_ngram_freq_1.update(unigrams_list_1)\n",
    "        text_ngram_freq_1.update(bigrams_list_1)\n",
    "        text_ngram_freq_1.update(trigrams_list_1)\n",
    "        text_features = [text_ngram_freq_1[ngrams] if ngrams in text_ngram_freq_1.keys() else 0 for ngrams in top_ngrams_1.keys()]\n",
    "        \n",
    "        text_ngram_freq_2 = Counter()\n",
    "        unigrams_list_2 = list(ngrams(tokens_2, 1))\n",
    "        bigrams_list_2 = list(ngrams(tokens_2, 2))\n",
    "        trigrams_list_2 = list(ngrams(tokens_2, 3))\n",
    "        text_ngram_freq_2.update(unigrams_list_2)\n",
    "        text_ngram_freq_2.update(bigrams_list_2)\n",
    "        text_ngram_freq_2.update(trigrams_list_2)\n",
    "        text_features.extend([text_ngram_freq_2[ngrams] if ngrams in text_ngram_freq_2.keys() else 0 for ngrams in top_ngrams_2.keys()])\n",
    "        \n",
    "        text_ngram_freq_3 = Counter()\n",
    "        unigrams_list_3 = list(ngrams(tokens_3, 1))\n",
    "        bigrams_list_3 = list(ngrams(tokens_3, 2))\n",
    "        trigrams_list_3 = list(ngrams(tokens_3, 3))\n",
    "        text_ngram_freq_3.update(unigrams_list_3)\n",
    "        text_ngram_freq_3.update(bigrams_list_3)\n",
    "        text_ngram_freq_3.update(trigrams_list_3)\n",
    "        text_features.extend([text_ngram_freq_3[ngrams] if ngrams in text_ngram_freq_3.keys() else 0 for ngrams in top_ngrams_3.keys()])\n",
    "        \n",
    "        text_ngram_freq_4 = Counter()\n",
    "        unigrams_list_4 = list(ngrams(tokens_4, 1))\n",
    "        bigrams_list_4 = list(ngrams(tokens_4, 2))\n",
    "        trigrams_list_4 = list(ngrams(tokens_4, 3))\n",
    "        text_ngram_freq_4.update(unigrams_list_4)\n",
    "        text_ngram_freq_4.update(bigrams_list_4)\n",
    "        text_ngram_freq_4.update(trigrams_list_4)\n",
    "        text_features.extend([text_ngram_freq_4[ngrams] if ngrams in text_ngram_freq_4.keys() else 0 for ngrams in top_ngrams_4.keys()])\n",
    "\n",
    "        text_ngram_freq_5 = Counter()\n",
    "        unigrams_list_5 = list(ngrams(tokens_5, 1))\n",
    "        bigrams_list_5 = list(ngrams(tokens_5, 2))\n",
    "        trigrams_list_5 = list(ngrams(tokens_5, 3))\n",
    "        text_ngram_freq_5.update(unigrams_list_5)\n",
    "        text_ngram_freq_5.update(bigrams_list_5)\n",
    "        text_ngram_freq_5.update(trigrams_list_5)\n",
    "        text_features.extend([text_ngram_freq_5[ngrams] if ngrams in text_ngram_freq_5.keys() else 0 for ngrams in top_ngrams_5.keys()])\n",
    "        \n",
    "        test_result.append(text_features)\n",
    "    \n",
    "        \n",
    "    column_name = ['partition_pos_ngram_'+ str(i+1) for i in range(5000)]\n",
    "    return pd.DataFrame(train_result, columns=column_name), pd.DataFrame(test_result, columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24251cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(prompts_list):\n",
    "    df = pd.DataFrame(columns=[\"prompt\", \"fold\", \"quadratic_weighted_kappa\", \"mean_absolute_error\", \"pearson_correlation_coefficient\",\n",
    "                              \"OSA_gender\", \"OSA_gender_p_value\", \"OSD_gender\", \"OSD_gender_p_value\", \"CSD_gender\", \"CSD_gender_p_value\", \"SMD_1_gender\", \"SMD_0_gender\", \"MAED_gender\",\n",
    "                              \"OSA_Economically_disadvantaged\", \"OSA_Economically_disadvantaged_p_value\", \"OSD_Economically_disadvantaged\", \"OSD_Economically_disadvantaged_p_value\", \"CSD_Economically_disadvantaged\", \"CSD_Economically_disadvantaged_p_value\", \"SMD_1_Economically_disadvantaged\", \"SMD_0_Economically_disadvantaged\", \"MAED_Economically_disadvantaged\",\n",
    "                              \"OSA_Disability\", \"OSA_Disability_p_value\", \"OSD_Disability\", \"OSD_Disability_p_value\", \"CSD_Disability\", \"CSD_Disability_p_value\", \"SMD_1_Disability\", \"SMD_0_Disability\", \"MAED_Disability\",\n",
    "                              \"OSA_English_Language_Learner\", \"OSA_English_Language_Learner_p_value\", \"OSD_English_Language_Learner\", \"OSD_English_Language_Learner_p_value\", \"CSD_English_Language_Learner\", \"CSD_English_Language_Learner_p_value\", \"SMD_1_English_Language_Learner\", \"SMD_0_English_Language_Learner\", \"MAED_English_Language_Learner\",\n",
    "                              \"OSA_Race\", \"OSA_Race_p_value\", \"OSD_Race\", \"OSD_Race_p_value\", \"CSD_Race\", \"CSD_Race_p_value\", \"SMD_1_Race\", \"SMD_0_Race\", \"MAED_Race\"])\n",
    "    for i in tqdm(range(len(prompts_list))):\n",
    "        kfolds = split_data(prompts_list[i][0], 5)\n",
    "        essay_length_feature = generate_feature(prompts_list[i][0])\n",
    "        features = pd.concat([essay_length_feature, prompts_list[i][1]], axis=1)\n",
    "        k = 0\n",
    "        for pair in tqdm(kfolds):\n",
    "            train_features_1, test_features_1 = generate_word_n_gram(prompts_list[i][0].iloc[pair[0]], prompts_list[i][0].iloc[pair[1]])\n",
    "            train_features_2, test_features_2 = generate_pos_n_gram(prompts_list[i][0].iloc[pair[0]], prompts_list[i][0].iloc[pair[1]])\n",
    "            train_features_3, test_features_3 = generate_partition_word_ngram(prompts_list[i][0].iloc[pair[0]], prompts_list[i][0].iloc[pair[1]])\n",
    "            X_train = pd.concat([train_features_1, train_features_2, train_features_3, features.iloc[pair[0]].reset_index(drop=True)], axis=1)\n",
    "            y_train = prompts_list[i][0].iloc[pair[0]]['Overall'].to_numpy()\n",
    "            X_test = pd.concat([test_features_1, test_features_2, test_features_3, features.iloc[pair[1]].reset_index(drop=True)], axis=1)\n",
    "            y_test = prompts_list[i][0].iloc[pair[1]]['Overall'].to_numpy()\n",
    "            test_info = prompts_list[i][0].iloc[pair[1]]\n",
    "            \n",
    "            ssc = StandardScaler()\n",
    "            X_train = ssc.fit_transform(X_train)\n",
    "            model = SVR()\n",
    "            model.fit(X_train, y_train)\n",
    "            X_test = ssc.transform(X_test)\n",
    "            \n",
    "            qwk, mae, pearson_score = accuracy_evaluation(model, X_test, y_test)\n",
    "            fairness_part1_Gender, fairness_part2_Gender = fairness_evaluation(model, X_test, y_test, test_info['Gender'].to_numpy())\n",
    "            fairness_part1_Economically_disadvantaged, fairness_part2_Economically_disadvantaged = fairness_evaluation(model, X_test, y_test, test_info['Economically_disadvantaged'].to_numpy())\n",
    "            fairness_part1_Disability, fairness_part2_Disability = fairness_evaluation(model, X_test, y_test, test_info['Disability'].to_numpy())\n",
    "            fairness_part1_English_Language_Learner, fairness_part2_English_Language_Learner = fairness_evaluation(model, X_test, y_test, test_info['English_Language_Learner'].to_numpy())\n",
    "            fairness_part1_Race, fairness_part2_Race = fairness_evaluation(model, X_test, y_test, test_info['Race_Binary'].to_numpy())\n",
    "            new_row = {\"prompt\" : i+1, \"fold\": k+1, \"quadratic_weighted_kappa\": qwk, \"mean_absolute_error\": mae, \"pearson_correlation_coefficient\": pearson_score,\n",
    "                      \"OSA_gender\": fairness_part1_Gender['Overall score accuracy']['R2'],\n",
    "                      \"OSA_gender_p_value\": fairness_part1_Gender['Overall score accuracy']['sig'],\n",
    "                      \"OSD_gender\": fairness_part1_Gender['Overall score difference']['R2'],\n",
    "                      \"OSD_gender_p_value\": fairness_part1_Gender['Overall score difference']['sig'],\n",
    "                      \"CSD_gender\": fairness_part1_Gender['Conditional score difference']['R2'],\n",
    "                      \"CSD_gender_p_value\": fairness_part1_Gender['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_gender\":fairness_part2_Gender['SMD_1'][0],\n",
    "                      \"SMD_0_gender\":fairness_part2_Gender['SMD_0'][0],\n",
    "                      \"MAED_gender\":fairness_part2_Gender['diff_mae'][0],\n",
    "                      \"OSA_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Overall score difference']['R2'],\n",
    "                      \"OSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Overall score difference']['sig'],\n",
    "                      \"CSD_Economically_disadvantaged\": fairness_part1_Economically_disadvantaged['Conditional score difference']['R2'],\n",
    "                      \"CSD_Economically_disadvantaged_p_value\": fairness_part1_Economically_disadvantaged['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_1'][0],\n",
    "                      \"SMD_0_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['SMD_0'][0],\n",
    "                      \"MAED_Economically_disadvantaged\":fairness_part2_Economically_disadvantaged['diff_mae'][0],\n",
    "                      \"OSA_Disability\": fairness_part1_Disability['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Disability_p_value\": fairness_part1_Disability['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Disability\": fairness_part1_Disability['Overall score difference']['R2'],\n",
    "                      \"OSD_Disability_p_value\": fairness_part1_Disability['Overall score difference']['sig'],\n",
    "                      \"CSD_Disability\": fairness_part1_Disability['Conditional score difference']['R2'],\n",
    "                      \"CSD_Disability_p_value\": fairness_part1_Disability['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Disability\":fairness_part2_Disability['SMD_1'][0],\n",
    "                      \"SMD_0_Disability\":fairness_part2_Disability['SMD_0'][0],\n",
    "                      \"MAED_Disability\":fairness_part2_Disability['diff_mae'][0],\n",
    "                      \"OSA_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score accuracy']['R2'],\n",
    "                      \"OSA_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score accuracy']['sig'],\n",
    "                      \"OSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Overall score difference']['R2'],\n",
    "                      \"OSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Overall score difference']['sig'],\n",
    "                      \"CSD_English_Language_Learner\": fairness_part1_English_Language_Learner['Conditional score difference']['R2'],\n",
    "                      \"CSD_English_Language_Learner_p_value\": fairness_part1_English_Language_Learner['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_1'][0],\n",
    "                      \"SMD_0_English_Language_Learner\":fairness_part2_English_Language_Learner['SMD_0'][0],\n",
    "                      \"MAED_English_Language_Learner\":fairness_part2_English_Language_Learner['diff_mae'][0],\n",
    "                      \"OSA_Race\": fairness_part1_Race['Overall score accuracy']['R2'],\n",
    "                      \"OSA_Race_p_value\": fairness_part1_Race['Overall score accuracy']['sig'],\n",
    "                      \"OSD_Race\": fairness_part1_Race['Overall score difference']['R2'],\n",
    "                      \"OSD_Race_p_value\": fairness_part1_Race['Overall score difference']['sig'],\n",
    "                      \"CSD_Race\": fairness_part1_Race['Conditional score difference']['R2'],\n",
    "                      \"CSD_Race_p_value\": fairness_part1_Race['Conditional score difference']['sig'],\n",
    "                      \"SMD_1_Race\":fairness_part2_Race['SMD_1'][0],\n",
    "                      \"SMD_0_Race\":fairness_part2_Race['SMD_0'][0],\n",
    "                      \"MAED_Race\":fairness_part2_Race['diff_mae'][0]}\n",
    "            k += 1\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    return df\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b79c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_data(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cross_validation(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1659b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508ab10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
